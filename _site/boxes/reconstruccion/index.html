<!doctype html>
<html lan="es">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <title>Reconstrucción 3D | NachoAz</title>
  <meta name="viewport" content="width=device-width,initial-scale=1">

  <link rel="shortcut icon" href="https://avatars3.githubusercontent.com/u/9017430?s=460&v=4
?s=16">
  <link rel="stylesheet" href="/assets/css/styles.css">
  <link href='http://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700|Ubuntu:300,500,300italic,500italic|Ubuntu+Condensed' rel='stylesheet' type='text/css'>

  <script type="text/javascript" charset="utf-8" src="/js/matchmedia.js"></script>
  <script type="text/javascript" charset="utf-8" src="/js/picturefill.js"></script>
</head>

<body>
<div class="wrapper">
    <div class="github">
      <a href="https://github.com/igarag/exemplar"><img style="position: absolute; top: 0; right: 0; border: 0;" alt="Fork me on GitHub"></a>
    </div>
    <header>
    <div class="container">
        <div class="profile">
            <div class="social">
                <a href="https://twitter.com/i__arranz">
                    <div data-picture data-alt="Twitter">
                        <div data-src="/img/twitter16.png"></div>
                        <div data-src="/img/twitter32.png"     data-media="(-webkit-min-device-pixel-ratio: 2)"></div>
                        <noscript><img src="/img/twitter16.png" alt="Twitter"></noscript>
                    </div>
                </a>
                <a href="https://github.com/igarag">
                    <div data-picture data-alt="Github">
                        <div data-src="/img/octocat16.png"></div>
                        <div data-src="/img/octocat32.png"     data-media="(-webkit-min-device-pixel-ratio: 2)"></div>
                        <noscript><img src="/img/octocat16.png" alt="Github"></noscript>
                    </div>
                </a>
            </div>
            <a href="https://github.com/igarag">
                <div class="avatar" data-picture data-alt="avatar">
                    <div data-src="https://avatars3.githubusercontent.com/u/9017430?s=460&v=4
?s=54"></div>
                    <div data-src="https://avatars3.githubusercontent.com/u/9017430?s=460&v=4
?s=108" data-media="(-webkit-min-device-pixel-ratio: 2)"></div>
                    <noscript><img src="https://avatars3.githubusercontent.com/u/9017430?s=460&v=4
?s=54" alt="avatar"></noscript>
                </div>
            </a>
        </div>
        <h5 class="logo"><a href="https://github.com/igarag">Nacho Az</a></h5>
        <nav class="main">
            <ul>
                
                
                <li><a href="https://igarag.github.io/online-cv/">Sobre mí</a></li>
                
                
                <li><a href="https://github.com/igarag">Proyectos</a></li>
                
            </ul>
        </nav>
        <div class="nav-slider">
            <div class="nav-selection "></div>
        </div>
        <div class="subnav-slider">
            <div class="subnav-selection "></div>
        </div>
        <nav class="subnav">
            <ul>
                
                    <li><a href="/" data-subnav="exemplar-home">Inicio</a></li>
                
                
                
                    <li><a href="https://github.com/igarag/mova-vision-robotica" data-subnav="download">Repositorio</a></li>
                
            </ul>
        </nav>
    </div>
</header>


    <section id="main">
        <article class="container">
          <h1 class="hero">Reconstrucción 3D</h1>
          <p>Esta segunda práctica de la asignatura de Visión en Robótica consiste en reconstruir una escena 3D a partir de las imágenes que recoge un robot a través de las cámaras de un par estéreo canónico. Utilizando las propiedades vistas durante el transcurso de la asignatura se pretende obtener correspondencias entre una imagen y la otra para obtener una recontrucción del punto en un espacio tridimensional ofrecido por la plataforma Unibotics.</p>

<p>Este proceso de reconstrucción, así como el código desarrollado, siguen una serie de etapas que se detallan a continuación:</p>

<h3 id="1-preproceso">1. Preproceso</h3>
<p>Partiendo como imagen fija la cámara izquierda, se buscan puntos de interés sobre la esta cámara. Estos puntos son obtenidos utilizando el filtro de Canny para obtener bordes. Inicialmente se parte de un umbral donde se detectan pocos puntos para que sea más cómodo trabajar en las primeras iteraciones con el programa. Posteriormente se incrementará el número de puntos como se verá más adelante.</p>

<p>Una vez se tienen estos puntos de interés representada por una nueva imagen con los píxeles con valor 255 donde existe un borde, comienza la siguiente etapa.</p>

<h3 id="2-retroproyección">2. Retroproyección</h3>
<p>Utilizando el API ofrecido por la plataforma, se toma un punto de los almacenados como puntos de interés para, pasarlo de coordenadas gráficas (píxeles) a coordenadas ópticas y retroproyectarlo.</p>

<pre><code class="language-python">pointInOpt = self.camLeftP.graficToOptical(point)
point1_3D_Left = self.camLeftP.backproject(pointInOpt)
</code></pre>

<p>El resultados de esta operación es un punto (no definido) en el espacio. Este punto, junto con el centro óptico de la cámara, unidos, forman el rayo de retroproyección. Será en este rayo donde se encuentre el punto correspondiente en la otra cámara, el cual se buscará a través de la <strong>línea epipolar</strong>.</p>

<h3 id="3-epipolar">3. Epipolar</h3>
<p>Para reducir el tiempo de búsqueda del punto correspondiente en la cámara derecha, se limita la búsqueda a una línea, la línea epipolar. Ese tiempo de búsqueda de la correspondencia se reduce aún más si se limita a unos pocos puntos de esa línea. Dado que la distancia entre las cámaras es pequeña, no será necesario buscar más allá de unos pocos píxeles. Para esta práctica, esa limitación está en 15 píxeles hacia la izquierda (como se busca en la cámara derecha, la mejor correspondencia estará hacia la izquierda).</p>

<div style="text-align: center">
    <img src="./img/epipolar.jpg" width="80%" height="60%" />
</div>

<h3 id="4-matching">4. Matching</h3>
<p>Para saber cómo de buena es la correspondencia se utiliza la función de openCV <code>matchTemplate</code> con la métrica <code>CCORR_NORMED</code> que devuelve una medida de correlación. Esta medida se realiza entre <strong>dos recortes</strong> tomados, uno de cada imagen. Fijando un recorte de la cámara izquierda, se extrae, en el mismo punto, uno en la imagen derecha y se mide el parentesco. Esto se repite desplazándose hacia la izquierda en busca la mejor medida de correspondencia. De los 15 píxeles de desplazamiento máximos se retornará de la función <code>matching</code> el que mayor correlación tenga. Ese será el recorte que mejor parentesco tiene con el correspondiente de la cámara izquierda y el centro del recorte será la coordenada del píxel que se retroproyecta en la siguiente etapa.</p>

<h3 id="5-nueva-retroproyección">5. Nueva retroproyección</h3>
<p>Con la coordenada de la cámara derecha del mejor píxel obtenido tras la correlación se repiten los pasos para obtener el punto en 3D que, de igual manera, unido con el centro óptico, se obtiene el nuevo rayo de retroproyección. Esta vez de la cámara derecha.</p>

<p>En el siguiente vídeo puede verse qué ocurre si representamos en el espacio los puntos de retroproyección en ambas cámaras. En color negro están los puntos retroproyectados de la cámara derecha y en blanco los de la cámara izquierda. Con esta disparidad se calcularán las interrecciones. Estas íntersecciones estarán más próximas a punto desde donde se está visualizando esta escena.</p>

<div style="text-align: center">
    <video width="800px" height="500px" controls="" preload="">
        <source src="./img/reconstruccion_trazas.mp4" />&lt;/source&gt; 
    </video>
</div>

<h3 id="6-intersección-entre-los-rayos-triangulación">6. Intersección entre los rayos, triangulación.</h3>
<p>Idealemente ambos rayos deberían coincidir en un punto común que coincidiría con el punto tridimensional. En la realidad, eso no ocurre, por lo que se tiene que buscar el punto donde la distancia entre dos rectas sea más pequeña. Para esta relación se utiliza las ecuaciones para obtener la distancia mínima entre dos rectas en <a href="http://paulbourke.net/geometry/pointlineplane/">este enlace</a>. Donde cada uno de los puntos representa la posición del centro óptico de la cámara izquierda, derecha así como los puntos retroproyectados. El punto medio entre las distancias será el punto final que se utilizará en la siguiente etapa.</p>

<h3 id="7-resultados">7. Resultados</h3>
<p>Una vez se completan los pasos anteriores, la última etapa es la representación en el Canvas 3D de la plataforma. Para ello se obtiene (usando las cooredenadas de la imagen izquierda) el color del pixel para que el resultado se obtenga en color. Pasados aproximadamente 90 segundos se obtiene el resultado que se puede ver en la figura.</p>

<div style="text-align: center">
    <img src="./img/solucion.png" width="80%" height="60%" />
</div>

<p>Puede verse el proceso de la reconstrucción en el siguiente vídeo:</p>

<p>Las zonas huecas que se ven en la solución (interior del patito de goma) ocurre porque no se han sacado puntos de interés en ese área (caen como si se trataran de fondo) y generan el hueco.</p>

<p>Si lo visualizamos desde otro ángulo, podemos ver los planos formados a distinta profundidad. Debido a la resolución de las imágenes que se obtienen desde las cámaras, se ven claramente los distintos planos de profundidad. Esto es debido a que, objetos más cercanos tienen más disparidad que los lejanos. Dado que la resolución de las imágenes es de 320x240 existen unas limitaciones a la hora de obtener correspondencias. A medida que la resolución de la cámara aumenta es posible obtener más planos en la disparidad, lo que se traduciría en mayor número de planos de imagen de los que se ven en la siguiente imagen.</p>

<p><img src="./img/plano_cenital.png" width="100%" height="60%" /></p>

<p>El número de puntos que se han representado son, aproximadamente, 23580 en 1:30 minutos.</p>

<h3 id="8-conclusiones">8. Conclusiones</h3>
<p>Se ha solucionado la práctica de reconstrucción 3D de una escena empleando dos cámaras en un par estéreo y midiendo, en la línea epipolar, el parentesco entre parches de las imágenes con el fin de obtener la triangulación y representarse en el espacio 3D.</p>

<p>Una vía para mejorar esta reconstrucción sería obtener más puntos ya que se obtendrían todos los píxeles de los objetos de la escena. Dado que sería muy posible que en este proceso se incluyera más ruido también sería necesaria una comprobación que descartara posibles puntos que no pertenecen a los objetos.</p>

<p>Como conlusión personal, tanto esta como la anterior, han sido prácticas muy interesantes de realizar que han permitido explotar la plataforma mostrando las ventajas de programar en un entorno web.</p>

        </article>
    </section>

    <footer>
    <nav class="container">
        <ul>
            <li>&copy; NachoAz 2019 - GtHub Pages - Jekyll</li>
        </ul>
    </nav>
</footer>

</body>
<script type="text/javascript">

    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-33073396-1']);
    _gaq.push(['_trackPageview']);

    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();

</script>
<script type="text/javascript" charset="utf-8">
    document.onreadystatechange = function() {
        if ( document.readyState == 'complete' ) {
            var active = document.querySelector( 'header nav.main a.active' ).innerHTML.toLowerCase(),
                links = document.querySelectorAll( 'header nav.main li a'),
                selection = document.querySelector( 'header .nav-selection' ),
                subactive = document.querySelector( 'header nav.subnav a.active' ).dataset.subnav,
                sublinks = document.querySelectorAll( 'header nav.subnav li a'),
                subselection = document.querySelector( 'header .subnav-selection' ),
                i, j, link;
            for ( i = 0; i < links.length; i++ ) {
                link = links[ i ];
                link.addEventListener( 'mouseover', function() {
                    selection.className = 'nav-selection';
                    selection.className = 'nav-selection ' + this.innerHTML.toLowerCase();
                });
                link.addEventListener( 'mouseout', function() {
                    selection.className = 'nav-selection ' + active;
                });
            }
            for ( i = 0; i < sublinks.length; i++ ) {
                link = sublinks[ i ];
                link.addEventListener( 'mouseover', function() {
                    subselection.className = 'subnav-selection';
                    subselection.className = 'subnav-selection ' + this.dataset.subnav;
                });
                link.addEventListener( 'mouseout', function() {
                    subselection.className = 'subnav-selection ' + subactive;
                });
            }
        }
    };
</script>

</html>
